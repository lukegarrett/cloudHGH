---
# This is a declarative approach to describe a Job type Kubernetes
# workload.   Since this is YAML, the indentation is very important
apiVersion: batch/v1
kind: Job         # We are testing the Job resource type
metadata:
  name: spark-job  # A name must be given to the Job type in the metadata section
spec:                     # This is the specification where we can even put the number of replicas
  template:               # Specified info needed to run the pod and what runs in the pod
    metadata:
      labels:
        app: spark        # some label to give to this pod (see the matching label above)
    spec:                 # actual specification
      containers:
      - name: spark       # this is going to be used for DNS name
        # Change the IP address to where you are running this
        image: camrenhall/cloudhgh:kbcon-image   # this is the image in registry accessible
                                             # from all our workers
        imagePullPolicy: Always  # This forces the node to pull the image
        command: ["spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 final_engine.py"]
                                                                                                       # iterations so that we have time to do an exec
                                                                                                       # and open a shell to the running pod
      restartPolicy: Never  # we ask Kubernetes not to restart
...



